{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brandon\\AppData\\Local\\anaconda3\\envs\\estnltk_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import os\n",
    "import fitz\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Funktsioon loomaks ja teostamaks päring vastavale lingile'''\n",
    "def koosta_paring(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'my_crawler (brandon.loorits@ut.ee) / for_study_purpose',\n",
    "    } # Määrame enda päringu päise, et oleks teada, kes päringuid veebileheküljele teeb\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f'Päring lehele {url} ebaõnnestus. Staatuskood: {response.status_code}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://nasdaqbaltic.com/statistics/et/shares'\n",
    "shares = []\n",
    "\n",
    "# Kogume kokku kõik url-id\n",
    "print(f'Külastan lehte: {main_url}')\n",
    "time.sleep(5)  # Viiteaeg, et ei ummistaks lehte\n",
    "page_content = koosta_paring(main_url)\n",
    "if page_content:\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    table = soup.find('table') \n",
    "    if table:\n",
    "        links = [a['href'] for a in table.find_all('a', href=True, class_=\"text16 compname\") if a['href'].startswith('/statistics/et/instrument')]\n",
    "        shares.extend(links)\n",
    "    else:\n",
    "        print('Ei leitud tabelit\".')\n",
    "else:\n",
    "    print('Algse lehe külastamine ebaõnnestus.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Külastame leitud linke\n",
    "reports=[]\n",
    "for link in shares:\n",
    "    absolute_link = f'https://nasdaqbaltic.com{link}'\n",
    "    print(f'Külastan lehte: {absolute_link}')\n",
    "    time.sleep(5)  # Ootame 5 sekundit enne järgmise päringu tegemist, et mitte ummistada lehekülge\n",
    "    page_content = koosta_paring(absolute_link)\n",
    "    if page_content:\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        link_element = soup.find('a', string=\"Aruanded\")\n",
    "        reports.append(link_element.get('href'))\n",
    "    else:\n",
    "        print(f'Lehe külastamine ebaõnnestus: {absolute_link}')\n",
    "print(len(reports))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_link =f'https://nasdaqbaltic.com'\n",
    "# Kõik failide lühendid, mis loetakse sisse XHTML failina kuna pdf-id puuduvad\n",
    "xhtml_nums = {\n",
    "    'ako':('pf57','pf38_1'),\n",
    "    'apg':('pf1','pf2c'),\n",
    "    'ign':('pfa3','pf136'),\n",
    "    'kne1':('pfa3','pf100'),\n",
    "    'pzv':('pf57','pf70'),\n",
    "    'rsu':('pf1','pf34'),\n",
    "    'sab':('pf1','pf4c'),\n",
    "    'vlp':('pf1','pf4b')\n",
    "}\n",
    "exclusion_list = xhtml_nums.keys()\n",
    "\n",
    "# Loome kaustad, kui need ei eksisteeri\n",
    "os.makedirs('aruanded/aastaaruanded', exist_ok=True)\n",
    "os.makedirs('alusfailid', exist_ok=True)\n",
    "\n",
    "# Otsime vajalikud aruanded ja salvestame need vastavasse kausta\n",
    "for report in reports:\n",
    "    absolute_link = f'{base_link}{report}'\n",
    "    print(f'Külastan lehte: {absolute_link}')\n",
    "    time.sleep(5)  # Ootame 5 sekundit enne järgmise päringu tegemist, et mitte ummistada lehekülge\n",
    "    page_content = koosta_paring(absolute_link)\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    table = soup.find('tbody')\n",
    "\n",
    "    pdf_link = None\n",
    "    zip_link = None\n",
    "    esg_link = None\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        links = row.find_all('a')\n",
    "        hrefs = [link.get('href') for link in links]\n",
    "        for href in hrefs:\n",
    "            if 'ar' in href.split('/')[-1] and href.endswith('.pdf') and href.split('/')[4] not in exclusion_list:\n",
    "                if href.split('/')[4] == 'dgr' and href.endswith('ias.pdf'): # erand ühele aruandele kuna sellel olemas ka lühendatud versioon, mida me ei vaja\n",
    "                    pdf_link = href\n",
    "                    break\n",
    "                else:\n",
    "                    pdf_link = href\n",
    "            elif 'ar' in href.split('/')[-1] and href.endswith('.zip') and not pdf_link:\n",
    "                zip_link = href\n",
    "            if 'esg' in href.split('/')[-1]:\n",
    "                esg_link = href\n",
    "        if pdf_link or zip_link:\n",
    "            break\n",
    "\n",
    "    # Kontrollime, kas aastaaruanne on olemas\n",
    "    if pdf_link:\n",
    "        # Kontrollime kõigepealt, kas ESG aruanne on olemas samale aastale\n",
    "        if esg_link:\n",
    "            esg_link_full = f'{base_link}{esg_link}'\n",
    "            print(f'ESG link: {esg_link_full}')\n",
    "            esg_response = requests.get(esg_link_full)\n",
    "            if esg_response:\n",
    "                match = re.search(r'/reports/([^_/]+/[^_/]+)_', esg_link_full).group(1).replace('/', '_')\n",
    "                print(f'Kirjutame esg {match} pdfi maha')\n",
    "                with open(f'aruanded/aastaaruanded/esg_{match}.pdf', 'wb') as file:\n",
    "                    file.write(esg_response.content)\n",
    "        else:# Kui ei ole, siis võtame href atribuudi väärtuse ja kirjutame aastaaruande kausta\n",
    "            rep_link = f'{base_link}{pdf_link}'\n",
    "            print(f'REP link: {rep_link}')\n",
    "                    \n",
    "            rep_response = requests.get(rep_link)\n",
    "            if rep_response:\n",
    "                match = re.search(r'/reports/([^_/]+/[^_/]+)_', rep_link).group(1).replace('/', '_')\n",
    "                print(f'Kirjutame aastaaruande pdfi {match} maha')\n",
    "                with open(f'aruanded/aastaaruanded/rep_{match}.pdf', 'wb') as file:\n",
    "                    file.write(rep_response.content)\n",
    "    elif zip_link:\n",
    "        # Võtame href atribuudi väärtuse, et hiljem ZIP kaustast leida aastaaruanne XHTML formaadis\n",
    "        rep_link = f'{base_link}{zip_link}'\n",
    "        print(f'REP link: {rep_link}')\n",
    "                \n",
    "        rep_response = requests.get(rep_link)\n",
    "        if rep_response:\n",
    "            match = re.search(r'/reports/([^_/]+/[^_/]+)_', rep_link).group(1).replace('/', '_')\n",
    "            print(f'Kirjutame aastaaruande zipi {match} maha')\n",
    "            with open(f'alusfailid/rep_{match}.zip', 'wb') as file:\n",
    "                file.write(rep_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Funktsioon, mis otsib aastaaruande tihendatud kaustast'''\n",
    "def leia_aruanne(zip_path, target_dir_rep):\n",
    "    # Kontrollime, kas väljundkaust on loodud\n",
    "    if not os.path.exists(target_dir_rep):\n",
    "        os.makedirs(target_dir_rep)\n",
    "    name = zip_path.split('\\\\')[1].replace('zip','xhtml')\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for file_name in zip_ref.namelist():\n",
    "            # Kontrollime, kas faili laiend on .xhtml \n",
    "            if file_name.endswith('.xhtml'):\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    content = file.read()\n",
    "                    \n",
    "                new_file_path = os.path.join(target_dir_rep, os.path.basename(name))\n",
    "                print(new_file_path)\n",
    "                # Kirjutame sisu uude faili\n",
    "                with open(new_file_path, 'wb') as new_file:\n",
    "                    new_file.write(content)\n",
    "                print(f\"Fail {os.path.basename(name)} on kirjutatud kausta {target_dir_rep}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_rep = 'aruanded/aruandedXHTML'\n",
    "\n",
    "zip_files = glob.glob('alusfailid/*.zip')\n",
    "\n",
    "for zip in zip_files:\n",
    "    leia_aruanne(zip,target_dir_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eralda_tekst_XHTML(xhtml_files, id_ranges):\n",
    "    for xhtml_file in xhtml_files:\n",
    "        file_name = xhtml_file.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "        # Leiame vajaliku osa infost failist ja eraldame teksti\n",
    "        if file_name in id_ranges:\n",
    "            start_id, end_id = id_ranges[file_name]\n",
    "\n",
    "            with open(xhtml_file, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            parsed_html = BeautifulSoup(content, 'lxml')\n",
    "            capture_text = False\n",
    "            extracted_text = \"\"\n",
    "\n",
    "            for div in parsed_html.find_all('div', id=True):\n",
    "                if div.get('id') == start_id:\n",
    "                    capture_text = True\n",
    "                if capture_text:\n",
    "                    extracted_text += div.get_text(separator=\"\\n\") + \"\\n\\n\"\n",
    "                if div.get('id') == end_id:\n",
    "                    capture_text = False  \n",
    "            # Salvesta tekst txt faili\n",
    "            output_file_path = xhtml_file.replace('.xhtml', '.txt').replace('aruandedXHTML','aastaaruanded')\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(extracted_text)\n",
    "            print(f\"Tekst failist {file_name}.xhtml märgndite {start_id} ja {end_id} vahel on salvestatud kausta: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aruandedXHTML = glob.glob('aruanded/aruandedXHTML/*')\n",
    "\n",
    "eralda_tekst_XHTML(aruandedXHTML, xhtml_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Funktsioon loomaks sisukordade sõnastik'''\n",
    "def loo_sisukordade_sonastik(paths):\n",
    "    toc_dict = {}\n",
    "    for path in paths:\n",
    "        file_name = path.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "        doc = fitz.open(path)\n",
    "        # Mustrid, mide leheküljelt otsitakse\n",
    "        toc_patterns = [\"sisukord\", \"table of contents\", \"content\", \"contents\"]\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            text_lines = doc[page_num].get_text().splitlines()\n",
    "            for line in text_lines:\n",
    "                line = line.strip().lower() \n",
    "                for pattern in toc_patterns:\n",
    "                    if line == pattern.lower():\n",
    "                        toc_dict[file_name] = page_num\n",
    "                        break  # Leidsime esimese vastavuse ja katkestame tsükli\n",
    "\n",
    "            if file_name in toc_dict:  # Kui oleme lehekülje leidnud, ei ole vaja edasi otsida\n",
    "                break\n",
    "    return toc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.pdf')\n",
    "toc = loo_sisukordade_sonastik(aastaaruanded)\n",
    "print('sisukorra lk nr:',toc)\n",
    "print(len(toc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leia_lk_numbrid(path, keywords, toc=None):\n",
    "    doc = fitz.open(path)\n",
    "    text_output_path = path.replace('.pdf', '.txt')\n",
    "    trim_start_page = None\n",
    "    file_name = path.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "    # Proovime leida, kas meil on leitud sisukord, kust leida lehekülgede vahemik, mida soovime\n",
    "    nbr = toc.get(file_name)\n",
    "    if nbr is not None:\n",
    "        toc_text = doc[nbr].get_text().replace('\\n',' ')\n",
    "        # Otsime vastavat mustrit sisukordadest, et saada soovitud lehekülgede vahemik\n",
    "        for keyword in keywords:\n",
    "            pattern = fr\"{re.escape(keyword)}\\s*(?:\\.+\\s*)+(\\d+)\"\n",
    "            match = re.search(pattern, toc_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                trim_start_page = int(match.group(1)) - 1\n",
    "                break  \n",
    "    # Kirjutame soovitud leheküljed uude faili tekstina\n",
    "    with open(text_output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        if trim_start_page is not None:\n",
    "            for page_num in range(trim_start_page):\n",
    "                page_text = doc[page_num].get_text()\n",
    "                text_file.write(page_text)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return trim_start_page\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.pdf')\n",
    "\n",
    "keywords = ['Konsolideeritud raamatupidamise aastaaruanne',\n",
    "'Kontserni raamatupidamise aastaaruanne',\n",
    "'Konsolideerimisgrupi raamatupidamise aastaaruanne',\n",
    "'RAAMATUPIDAMISE AASTAARUANNE',\n",
    "'Consolidated and separate financial statements',\n",
    "'Financial Statements']\n",
    "cuts = {}\n",
    "for aruanne in aastaaruanded:\n",
    "    print()\n",
    "    print('ARUANNE:',aruanne)\n",
    "    file_name = aruanne.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "    cut = leia_lk_numbrid(aruanne,keywords,toc)\n",
    "    print(cut)\n",
    "    cuts[file_name] = cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_problematic_pdf(path,cut):\n",
    "    doc = fitz.open(path)\n",
    "    text_output_path = path.replace('.pdf', '.txt')\n",
    "\n",
    "    with open(text_output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        for page_num in range(cut):\n",
    "            page_text = doc[page_num].get_text()\n",
    "            text_file.write(page_text)\n",
    "\n",
    "    doc.close()\n",
    "    # print(trim_start_page)\n",
    "    return text_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aastaaruanded, mille puhul on raskendatud sisukorrast lehekülgede leidmine, kuna ei ole mustrit, mille järgi otsida\n",
    "problematic_reports = {\n",
    "    'egr':89, \n",
    "    'hae':87, \n",
    "    'inf':14, \n",
    "    'lhv':81, \n",
    "    'ntu':41, \n",
    "    'saf':5, \n",
    "    'tel1':134, \n",
    "    'tsm':75\n",
    "}\n",
    "\n",
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.pdf')\n",
    "\n",
    "for aruanne in aastaaruanded:\n",
    "    for firm,cut in problematic_reports.items():\n",
    "        if firm in aruanne:\n",
    "            trim_problematic_pdf(aruanne,cut)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puhasta_tekst(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Eemaldame üleliigsed tühikud\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Liidame silbitatud sõnad\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Eemaldame mitteolulised sümbolid gpt mudeli jaoks\n",
    "    # text = re.sub(r'[^a-zA-Z0-9.,;:!?()\"\\']+', ' ', text)\n",
    "\n",
    "    # Kirjutame faili sisu üle\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.txt')\n",
    "\n",
    "for aruanne in aastaaruanded:\n",
    "    puhasta_tekst(aruanne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "'''Kood leidmaks tokenite umbkaudne (mudelid on erinevad) arv'''\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "txt_files = glob.glob('aruanded/aastaaruanded/*.txt')\n",
    "token_counts = {}\n",
    "\n",
    "\n",
    "for file_path in txt_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    cleaned_text = text\n",
    "    file_name = file_path.split('/')[-1]\n",
    "\n",
    "    tokens = tokenizer.tokenize(cleaned_text)\n",
    "    \n",
    "    token_counts[file_name] = len(tokens)\n",
    "\n",
    "print(f\"Tokenite arv: {token_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_token_counts = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for file_name, token_count in sorted_token_counts:\n",
    "    print(f\"{file_name}: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esg_data_path = os.getenv('ESG_INPUT')\n",
    "# esg_data = pd.read_excel(esg_data_path)\n",
    "# grouped_data = esg_data.groupby(['Topic', 'Question'])['Answers'].apply(list).reset_index()\n",
    "# order_of_topics = [\n",
    "#     'Stakeholder engagement and reporting',\n",
    "#     'Leadership commitment', \n",
    "#     'Impact assesment', \n",
    "#     'Planning', \n",
    "#     'Execution', \n",
    "#     'Monitoring', \n",
    "#     'Performance improvement', \n",
    "#     'Across all topics'\n",
    "# ]\n",
    "\n",
    "# grouped_data['Topic'] = pd.Categorical(grouped_data['Topic'], categories=order_of_topics, ordered=True)\n",
    "# sorted_data = grouped_data.sort_values('Topic')\n",
    "# sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esg_val_path = os.getenv('ESG_VAL')\n",
    "# esg_val = pd.read_excel(esg_val_path)\n",
    "# esg_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_text(row, answer_column):\n",
    "    if pd.notna(row[answer_column]):\n",
    "        answer_number = int(row[answer_column]) - 1 \n",
    "        if answer_number >= 0 and answer_number < len(row['Answers']):\n",
    "            return row['Answers'][answer_number]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data = esg_val.merge(sorted_data, on='Question', how='left')\n",
    "# merged_data['SS Explained'] = None\n",
    "# merged_data['GPT Explained'] = None\n",
    "# column_order = ['Company', 'Abbreviation','Topic', 'Question', 'SS Answer', 'SS Explained', 'GPT ANSWER 1','GPT Explained',  'Answers']\n",
    "# merged_data = merged_data[column_order]\n",
    "\n",
    "# merged_data['SS Explained'] = merged_data.apply(lambda row: get_answer_text(row, 'SS Answer'), axis=1)\n",
    "\n",
    "# merged_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estnltk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
