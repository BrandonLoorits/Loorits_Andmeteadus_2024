{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brandon\\AppData\\Local\\anaconda3\\envs\\estnltk_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import os\n",
    "import fitz\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Funktsioon loomaks ja teostamaks päring vastavale lingile'''\n",
    "def koosta_paring(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'my_crawler (brandon.loorits@ut.ee) / for_study_purpose',\n",
    "    } # Määrame enda päringu päise, et oleks teada, kes päringuid veebileheküljele teeb\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f'Päring lehele {url} ebaõnnestus. Staatuskood: {response.status_code}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://nasdaqbaltic.com/statistics/et/shares'\n",
    "shares = []\n",
    "\n",
    "# Kogume kokku kõik url-id\n",
    "print(f'Külastan lehte: {main_url}')\n",
    "time.sleep(5)  # Viiteaeg, et ei ummistaks lehte\n",
    "page_content = koosta_paring(main_url)\n",
    "if page_content:\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    table = soup.find('table') \n",
    "    if table:\n",
    "        links = [a['href'] for a in table.find_all('a', href=True, class_=\"text16 compname\") if a['href'].startswith('/statistics/et/instrument')]\n",
    "        shares.extend(links)\n",
    "    else:\n",
    "        print('Ei leitud tabelit\".')\n",
    "else:\n",
    "    print('Algse lehe külastamine ebaõnnestus.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Külastame leitud linke\n",
    "reports=[]\n",
    "for link in shares:\n",
    "    absolute_link = f'https://nasdaqbaltic.com{link}'\n",
    "    print(f'Külastan lehte: {absolute_link}')\n",
    "    time.sleep(5)  # Ootame 5 sekundit enne järgmise päringu tegemist, et mitte ummistada lehekülge\n",
    "    page_content = koosta_paring(absolute_link)\n",
    "    if page_content:\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        link_element = soup.find('a', string=\"Aruanded\")\n",
    "        reports.append(link_element.get('href'))\n",
    "    else:\n",
    "        print(f'Lehe külastamine ebaõnnestus: {absolute_link}')\n",
    "print(len(reports))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_link =f'https://nasdaqbaltic.com'\n",
    "# Kõik failide lühendid, mis loetakse sisse XHTML failina kuna pdf-id puuduvad\n",
    "xhtml_nums = {\n",
    "    'ako':('pf57','pf38_1'),\n",
    "    'apg':('pf1','pf2c'),\n",
    "    'ign':('pfa3','pf136'),\n",
    "    'kne1':('pfa3','pf100'),\n",
    "    'pzv':('pf57','pf70'),\n",
    "    'rsu':('pf1','pf34'),\n",
    "    'sab':('pf1','pf4c'),\n",
    "    'vlp':('pf1','pf4b')\n",
    "}\n",
    "exclusion_list = xhtml_nums.keys()\n",
    "\n",
    "# Loome kaustad, kui need ei eksisteeri\n",
    "os.makedirs('aruanded/aastaaruanded', exist_ok=True)\n",
    "os.makedirs('alusfailid', exist_ok=True)\n",
    "\n",
    "# Otsime vajalikud aruanded ja salvestame need vastavasse kausta\n",
    "for report in reports:\n",
    "    absolute_link = f'{base_link}{report}'\n",
    "    print(f'Külastan lehte: {absolute_link}')\n",
    "    time.sleep(5)  # Ootame 5 sekundit enne järgmise päringu tegemist, et mitte ummistada lehekülge\n",
    "    page_content = koosta_paring(absolute_link)\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    table = soup.find('tbody')\n",
    "\n",
    "    pdf_link = None\n",
    "    zip_link = None\n",
    "    esg_link = None\n",
    "\n",
    "    for row in table.find_all('tr'):\n",
    "        links = row.find_all('a')\n",
    "        hrefs = [link.get('href') for link in links]\n",
    "        for href in hrefs:\n",
    "            if 'ar' in href.split('/')[-1] and href.endswith('.pdf') and href.split('/')[4] not in exclusion_list:\n",
    "                if href.split('/')[4] == 'dgr' and href.endswith('ias.pdf'): # erand ühele aruandele kuna sellel olemas ka lühendatud versioon, mida me ei vaja\n",
    "                    pdf_link = href\n",
    "                    break\n",
    "                else:\n",
    "                    pdf_link = href\n",
    "            elif 'ar' in href.split('/')[-1] and href.endswith('.zip') and not pdf_link:\n",
    "                zip_link = href\n",
    "            if 'esg' in href.split('/')[-1]:\n",
    "                esg_link = href\n",
    "        if pdf_link or zip_link:\n",
    "            break\n",
    "\n",
    "    # Kontrollime, kas aastaaruanne on olemas\n",
    "    if pdf_link:\n",
    "        # Kontrollime kõigepealt, kas ESG aruanne on olemas samale aastale\n",
    "        if esg_link:\n",
    "            esg_link_full = f'{base_link}{esg_link}'\n",
    "            print(f'ESG link: {esg_link_full}')\n",
    "            esg_response = requests.get(esg_link_full)\n",
    "            if esg_response:\n",
    "                match = re.search(r'/reports/([^_/]+/[^_/]+)_', esg_link_full).group(1).replace('/', '_')\n",
    "                print(f'Kirjutame esg {match} pdfi maha')\n",
    "                with open(f'aruanded/aastaaruanded/esg_{match}.pdf', 'wb') as file:\n",
    "                    file.write(esg_response.content)\n",
    "        else:# Kui ei ole, siis võtame href atribuudi väärtuse ja kirjutame aastaaruande kausta\n",
    "            rep_link = f'{base_link}{pdf_link}'\n",
    "            print(f'REP link: {rep_link}')\n",
    "                    \n",
    "            rep_response = requests.get(rep_link)\n",
    "            if rep_response:\n",
    "                match = re.search(r'/reports/([^_/]+/[^_/]+)_', rep_link).group(1).replace('/', '_')\n",
    "                print(f'Kirjutame aastaaruande pdfi {match} maha')\n",
    "                with open(f'aruanded/aastaaruanded/rep_{match}.pdf', 'wb') as file:\n",
    "                    file.write(rep_response.content)\n",
    "    elif zip_link:\n",
    "        # Võtame href atribuudi väärtuse, et hiljem ZIP kaustast leida aastaaruanne XHTML formaadis\n",
    "        rep_link = f'{base_link}{zip_link}'\n",
    "        print(f'REP link: {rep_link}')\n",
    "                \n",
    "        rep_response = requests.get(rep_link)\n",
    "        if rep_response:\n",
    "            match = re.search(r'/reports/([^_/]+/[^_/]+)_', rep_link).group(1).replace('/', '_')\n",
    "            print(f'Kirjutame aastaaruande zipi {match} maha')\n",
    "            with open(f'alusfailid/rep_{match}.zip', 'wb') as file:\n",
    "                file.write(rep_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Funktsioon, mis otsib aastaaruande tihendatud kaustast'''\n",
    "def leia_aruanne(zip_path, target_dir_rep):\n",
    "    # Kontrollime, kas väljundkaust on loodud\n",
    "    if not os.path.exists(target_dir_rep):\n",
    "        os.makedirs(target_dir_rep)\n",
    "    name = zip_path.split('\\\\')[1].replace('zip','xhtml')\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for file_name in zip_ref.namelist():\n",
    "            # Kontrollime, kas faili laiend on .xhtml \n",
    "            if file_name.endswith('.xhtml'):\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    content = file.read()\n",
    "                    \n",
    "                new_file_path = os.path.join(target_dir_rep, os.path.basename(name))\n",
    "                print(new_file_path)\n",
    "                # Kirjutame sisu uude faili\n",
    "                with open(new_file_path, 'wb') as new_file:\n",
    "                    new_file.write(content)\n",
    "                print(f\"Fail {os.path.basename(name)} on kirjutatud kausta {target_dir_rep}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_rep = 'aruanded/aruandedXHTML'\n",
    "\n",
    "zip_files = glob.glob('alusfailid/*.zip')\n",
    "\n",
    "for zip in zip_files:\n",
    "    leia_aruanne(zip,target_dir_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eralda_tekst_XHTML(xhtml_files, id_ranges):\n",
    "    for xhtml_file in xhtml_files:\n",
    "        file_name = xhtml_file.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "        # Leiame vajaliku osa infost failist ja eraldame teksti\n",
    "        if file_name in id_ranges:\n",
    "            start_id, end_id = id_ranges[file_name]\n",
    "\n",
    "            with open(xhtml_file, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            parsed_html = BeautifulSoup(content, 'lxml')\n",
    "            capture_text = False\n",
    "            extracted_text = \"\"\n",
    "\n",
    "            for div in parsed_html.find_all('div', id=True):\n",
    "                if div.get('id') == start_id:\n",
    "                    capture_text = True\n",
    "                if capture_text:\n",
    "                    extracted_text += div.get_text(separator=\"\\n\") + \"\\n\\n\"\n",
    "                if div.get('id') == end_id:\n",
    "                    capture_text = False  \n",
    "            # Salvesta tekst txt faili\n",
    "            output_file_path = xhtml_file.replace('.xhtml', '.txt').replace('aruandedXHTML','aastaaruanded')\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(extracted_text)\n",
    "            print(f\"Tekst failist {file_name}.xhtml märgndite {start_id} ja {end_id} vahel on salvestatud kausta: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aruandedXHTML = glob.glob('aruanded/aruandedXHTML/*')\n",
    "\n",
    "eralda_tekst_XHTML(aruandedXHTML, xhtml_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Funktsioon loomaks sisukordade sõnastik'''\n",
    "def loo_sisukordade_sonastik(paths):\n",
    "    toc_dict = {}\n",
    "    for path in paths:\n",
    "        file_name = path.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "        doc = fitz.open(path)\n",
    "        # Mustrid, mide leheküljelt otsitakse\n",
    "        toc_patterns = [\"sisukord\", \"table of contents\", \"content\", \"contents\"]\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            text_lines = doc[page_num].get_text().splitlines()\n",
    "            for line in text_lines:\n",
    "                line = line.strip().lower() \n",
    "                for pattern in toc_patterns:\n",
    "                    if line == pattern.lower():\n",
    "                        toc_dict[file_name] = page_num\n",
    "                        break  # Leidsime esimese vastavuse ja katkestame tsükli\n",
    "\n",
    "            if file_name in toc_dict:  # Kui oleme lehekülje leidnud, ei ole vaja edasi otsida\n",
    "                break\n",
    "    return toc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.pdf')\n",
    "toc = loo_sisukordade_sonastik(aastaaruanded)\n",
    "print('sisukorra lk nr:',toc)\n",
    "print(len(toc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leia_lk_numbrid(path, keywords, toc=None):\n",
    "    doc = fitz.open(path)\n",
    "    text_output_path = path.replace('.pdf', '.txt')\n",
    "    trim_start_page = None\n",
    "    file_name = path.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "    # Proovime leida, kas meil on leitud sisukord, kust leida lehekülgede vahemik, mida soovime\n",
    "    nbr = toc.get(file_name)\n",
    "    if nbr is not None:\n",
    "        toc_text = doc[nbr].get_text().replace('\\n',' ')\n",
    "        # Otsime vastavat mustrit sisukordadest, et saada soovitud lehekülgede vahemik\n",
    "        for keyword in keywords:\n",
    "            pattern = fr\"{re.escape(keyword)}\\s*(?:\\.+\\s*)+(\\d+)\"\n",
    "            match = re.search(pattern, toc_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                trim_start_page = int(match.group(1)) - 1\n",
    "                break  \n",
    "    # Kirjutame soovitud leheküljed uude faili tekstina\n",
    "    with open(text_output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        if trim_start_page is not None:\n",
    "            for page_num in range(trim_start_page):\n",
    "                page_text = doc[page_num].get_text()\n",
    "                text_file.write(page_text)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return trim_start_page\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.pdf')\n",
    "\n",
    "keywords = ['Konsolideeritud raamatupidamise aastaaruanne',\n",
    "'Kontserni raamatupidamise aastaaruanne',\n",
    "'Konsolideerimisgrupi raamatupidamise aastaaruanne',\n",
    "'RAAMATUPIDAMISE AASTAARUANNE',\n",
    "'Consolidated and separate financial statements',\n",
    "'Financial Statements']\n",
    "cuts = {}\n",
    "for aruanne in aastaaruanded:\n",
    "    print()\n",
    "    print('ARUANNE:',aruanne)\n",
    "    file_name = aruanne.split('\\\\')[-1].split('.')[0].split('_')[1]\n",
    "    cut = leia_lk_numbrid(aruanne,keywords,toc)\n",
    "    print(cut)\n",
    "    cuts[file_name] = cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_problematic_pdf(path,cut):\n",
    "    doc = fitz.open(path)\n",
    "    text_output_path = path.replace('.pdf', '.txt')\n",
    "\n",
    "    with open(text_output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        for page_num in range(cut):\n",
    "            page_text = doc[page_num].get_text()\n",
    "            text_file.write(page_text)\n",
    "\n",
    "    doc.close()\n",
    "    # print(trim_start_page)\n",
    "    return text_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aastaaruanded, mille puhul on raskendatud sisukorrast lehekülgede leidmine, kuna ei ole mustrit, mille järgi otsida\n",
    "problematic_reports = {\n",
    "    'egr':89, \n",
    "    'hae':87, \n",
    "    'inf':14, \n",
    "    'lhv':81, \n",
    "    'ntu':41, \n",
    "    'saf':5, \n",
    "    'tel1':134, \n",
    "    'tsm':75\n",
    "}\n",
    "\n",
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.pdf')\n",
    "\n",
    "for aruanne in aastaaruanded:\n",
    "    for firm,cut in problematic_reports.items():\n",
    "        if firm in aruanne:\n",
    "            trim_problematic_pdf(aruanne,cut)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puhasta_tekst(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Eemaldame üleliigsed tühikud\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Liidame silbitatud sõnad\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Eemaldame mitteolulised sümbolid gpt mudeli jaoks\n",
    "    # text = re.sub(r'[^a-zA-Z0-9.,;:!?()\"\\']+', ' ', text)\n",
    "\n",
    "    # Kirjutame faili sisu üle\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aastaaruanded = glob.glob('aruanded/aastaaruanded/*.txt')\n",
    "\n",
    "for aruanne in aastaaruanded:\n",
    "    puhasta_tekst(aruanne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "'''Kood leidmaks tokenite umbkaudne (mudelid on erinevad) arv'''\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "txt_files = glob.glob('aruanded/aastaaruanded/*.txt')\n",
    "token_counts = {}\n",
    "\n",
    "\n",
    "for file_path in txt_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    cleaned_text = text\n",
    "    file_name = file_path.split('/')[-1]\n",
    "\n",
    "    tokens = tokenizer.tokenize(cleaned_text)\n",
    "    \n",
    "    token_counts[file_name] = len(tokens)\n",
    "\n",
    "print(f\"Tokenite arv: {token_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_token_counts = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for file_name, token_count in sorted_token_counts:\n",
    "    print(f\"{file_name}: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JÄRGENVAID KOODIRIDASID EI SAA KÄIVITADA KUI POLE ALUSFAILE\n",
    "\n",
    "KOOSTÖÖD TEINUD ETTEVÕTE KONFIDENTSIAALSUSE REEGLITE TÕTTU EI SAA JAGADA ALUSFAILE\n",
    "\n",
    "JÄRGNEVAD KOODIRIDASID KASUTATI LÕPUTÖÖS SAADUD TULEMUSTE SAAMISEKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esg_data_path = os.getenv('ESG_INPUT')\n",
    "# esg_data = pd.read_excel(esg_data_path)\n",
    "# grouped_data = esg_data.groupby(['Topic', 'Question'])['Answers'].apply(list).reset_index()\n",
    "# order_of_topics = [\n",
    "#     'Stakeholder engagement and reporting',\n",
    "#     'Leadership commitment', \n",
    "#     'Impact assesment', \n",
    "#     'Planning', \n",
    "#     'Execution', \n",
    "#     'Monitoring', \n",
    "#     'Performance improvement', \n",
    "#     'Across all topics'\n",
    "# ]\n",
    "\n",
    "# grouped_data['Topic'] = pd.Categorical(grouped_data['Topic'], categories=order_of_topics, ordered=True)\n",
    "# sorted_data = grouped_data.sort_values('Topic')\n",
    "# sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esg_val_path = os.getenv('ESG_VAL')\n",
    "# esg_val = pd.read_excel(esg_val_path)\n",
    "# esg_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_text(row, answer_column):\n",
    "    if pd.notna(row[answer_column]):\n",
    "        answer_number = int(row[answer_column]) - 1 \n",
    "        if answer_number >= 0 and answer_number < len(row['Answers']):\n",
    "            return row['Answers'][answer_number]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data = esg_val.merge(sorted_data, on=['Question','Topic'], how='left')\n",
    "\n",
    "# merged_data['SS Explained'] = None\n",
    "# merged_data['GPT Explained'] = None\n",
    "# column_order = ['Company', 'Abbreviation', 'Topic', 'Factor', 'Question', 'SS Answer', 'SS Explained', 'GPT Answer 1','GPT Explained',  'Answers']\n",
    "# merged_data = merged_data[column_order]\n",
    "\n",
    "# merged_data['SS Explained'] = merged_data.apply(lambda row: get_answer_text(row, 'SS Answer'), axis=1)\n",
    "\n",
    "# merged_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"merged_data3.xlsx\"\n",
    "# if os.path.isfile(file_path):\n",
    "#     # Loeme faili sisse DataFrame'i\n",
    "#     merged_data = pd.read_excel(file_path)\n",
    "#     merged_data['Answers'] = merged_data['Answers'].apply(eval)\n",
    "\n",
    "# merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def api_paring(txt_files, answer_column_name,pre_prompt):\n",
    "    client = openai.OpenAI(api_key=os.getenv('MY_API_KEY'))\n",
    "    # Kui veergu pole olemas, siis see luuakse\n",
    "    if answer_column_name not in merged_data.columns:\n",
    "        merged_data[answer_column_name] = None\n",
    "        \n",
    "    for file in txt_files:\n",
    "        file_name = os.path.basename(file.replace('\\\\', '/')) \n",
    "        abbrev = file_name.split('_')[1]  \n",
    "        \n",
    "        with open(file, 'r', encoding='utf-8') as file:\n",
    "            user_input_text = file.read()\n",
    "        \n",
    "        print(\"Abbreviation:\", abbrev.upper())\n",
    "        data_slice = merged_data[merged_data['Abbreviation'] == abbrev.upper()]\n",
    "        topics = data_slice.groupby('Topic', sort=False)\n",
    "        \n",
    "        for topic, group in topics:\n",
    "            system_prompt = pre_prompt\n",
    "            if topic == 'Stakeholder engagement and reporting':\n",
    "                system_prompt += f\"Give only one answer in format 'question:answer number', example: 'What number is best?:1', nothing more - only question and answer.\\n\"\n",
    "                system_prompt += f\"Topic: {topic}\\n\"\n",
    "                \n",
    "                for index, row in group.iterrows():\n",
    "                    system_prompt = pre_prompt\n",
    "                    system_prompt += f\"Give only one answer in format 'question:answer number', example: 'What number is best?:1', nothing more - only question and answer.\\n\"\n",
    "                    system_prompt += f\"Topic: {topic}\\n\"\n",
    "                    system_prompt += f\"Questions:\\n{row['Question']}\\n\"\n",
    "                    # print(row['Answers'])\n",
    "                    answers = [f\"{a}\" for a in row['Answers']] \n",
    "                    system_prompt +=\"Answers\\n\" + \"\\n\".join(answers)\n",
    "                    # print(row['Question'] == 'What is the customer’s scope regarding Stakeholder engagement & reporting?')\n",
    "                    print(\"System Prompt:\\n\", system_prompt)\n",
    "                    chat_completion = client.chat.completions.create(\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_input_text}\n",
    "                        ],\n",
    "                        model=\"gpt-4-turbo\",  \n",
    "                    )\n",
    "                    \n",
    "                    for choice in chat_completion.choices:\n",
    "                        if choice.message.role == 'assistant':\n",
    "                            answers = choice.message.content.strip().split('\\n')\n",
    "                            for i, answer in enumerate(answers):\n",
    "                                q, a = answer.split(':')\n",
    "                                print(q, a)\n",
    "                                print(group.index[i])\n",
    "                                if row['Question'] == 'What is the customer’s scope regarding Stakeholder engagement & reporting?':\n",
    "                                    merged_data.at[group.index[i+1], answer_column_name] = float(a.strip())\n",
    "                                else:\n",
    "                                    merged_data.at[group.index[i], answer_column_name] = float(a.strip())\n",
    "            \n",
    "            else:\n",
    "                system_prompt += f\"Give only one answer for every factor in format 'factor:answer number', example: 'Employee Safety:1', nothing more - only factor and answer.\\n\"\n",
    "                system_prompt += f\"Topic: {topic}\\n\"\n",
    "                questions = \"\\n\".join(group['Question'].unique())\n",
    "                system_prompt += f\"Question/s:\\n{questions}\\n\"\n",
    "                factors = \"\\n\".join(group['Factor'].unique())\n",
    "                system_prompt += f\"Factors:\\n{factors}\\n\"\n",
    "                answers = [f\"{a}\" for a in group['Answers'].iloc[0]] \n",
    "                system_prompt +=\"Answers\\n\" + \"\\n\".join(answers)\n",
    "                \n",
    "                print(\"System Prompt:\\n\", system_prompt)\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_input_text}\n",
    "                    ],\n",
    "                    model=\"gpt-4-turbo\",  \n",
    "                )\n",
    "\n",
    "                for choice in chat_completion.choices:\n",
    "                    if choice.message.role == 'assistant':\n",
    "                        answers = choice.message.content.strip().split('\\n')\n",
    "                        for i, answer in enumerate(answers):\n",
    "                            q, a = answer.split(':')\n",
    "                            print(q, a)\n",
    "                            print(group.index[i])\n",
    "                            merged_data.at[group.index[i], answer_column_name] = float(a.strip())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viimane_katse_nbr(df):\n",
    "    # Leia veergude nimed, mis algavad \"GPT Answer\"ga\n",
    "    gpt_answer_columns = df.filter(like='GPT Answer').columns\n",
    "\n",
    "    # Alusta maksimaalse arvu leidmist nullist\n",
    "    max_gpt_answer_number = 0\n",
    "\n",
    "    # Itereeri läbi kõigi leitud veergude ja leia maksimaalne number\n",
    "    for column in gpt_answer_columns:\n",
    "        # Eralda veeru numbri osa ja muuda see täisarvuks\n",
    "        column_number = int(column.split(' ')[-1])\n",
    "        if column_number > max_gpt_answer_number:\n",
    "            max_gpt_answer_number = column_number\n",
    "\n",
    "    return max_gpt_answer_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leia_valideerimis_hulk(txt_files, merged_data):\n",
    "    # Loend failidest, millel on vähemalt üks Abbreviation väärtus\n",
    "    filtered_files = []\n",
    "    \n",
    "    for file in txt_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        abbrev = file_name.split('_')[1].replace('.txt', '').upper()\n",
    "        if abbrev in merged_data['Abbreviation'].unique():\n",
    "            filtered_files.append(file)\n",
    "    \n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('Loorits_Andmeteadus_2024/aruanded/aastaaruanded/*.txt')\n",
    "# txt_files = leia_valideerimis_hulk(files,merged_data)\n",
    "# prompt = (\n",
    "#                 \"Act as a sustainability specialist, who is filling in the pre-analysis questions based on the SASB methodology. \"\n",
    "#                 \"Focus on finding the relevant parts of text based on the topic and then try to answer. \\n\"\n",
    "#                 \"Choose one of the answer options, which is most suitable according to the provided report.\\n\"\n",
    "#                 \"Ensure that for each question and factor, one precise answer must be provided. It is not possible to leave any question or factor unanswered.\\n\"\n",
    "#             )\n",
    "# # print(txt_files)\n",
    "# last_max = viimane_katse_nbr(merged_data)\n",
    "# api_paring(txt_files, f'GPT Answer {last_max+1}',prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = \"C:/Users/brandon/Desktop/maka töö/merged_data3.xlsx\"\n",
    "merged_data.to_excel(destination_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_matching(row,col1,col2):\n",
    "    return row[col1] == row[col2]\n",
    "\n",
    "def check_unmatching(row,col1,col2):\n",
    "    return row[col1] != row[col2]\n",
    "\n",
    "def check_bigger_than(row,col1,col2):\n",
    "    if row[col1] is not None and row[col2] is not None:\n",
    "        return row[col1] > row[col2]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_smaller_than(row,col1,col2):\n",
    "    if row[col1] is not None and row[col2] is not None:\n",
    "        return row[col1] < row[col2]\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_closeness(row,col1,col2):\n",
    "    if row[col1] is not None and row[col2] is not None:\n",
    "        return abs(row[col1] - row[col2]) == 1 or row[col1] == row[col2]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_none_val(row,col1):\n",
    "    return row[col1] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column(df,col1,col2,res_col, check_func):\n",
    "    df[res_col] = df.apply(lambda row: check_func(row, col1, col2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.replace({np.nan: None})\n",
    "merged_data['GPT2 None'] = merged_data.apply(lambda row: check_none_val(row, 'GPT Answer 2'), axis=1)\n",
    "merged_data['GPT4 None'] = merged_data.apply(lambda row: check_none_val(row, 'GPT Answer 4'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_column(merged_data,'SS Answer','GPT Answer 4','SS=GPT4',check_matching)\n",
    "# add_column(merged_data,'SS Answer','GPT Answer 4','UnMatching4',check_unmatching)\n",
    "add_column(merged_data,'SS Answer','GPT Answer 4','SS>GPT4',check_bigger_than)\n",
    "# add_column(merged_data,'SS Answer','GPT Answer 4','Smaller4',check_smaller_than)\n",
    "add_column(merged_data,'SS Answer','GPT Answer 4','SS#GPT4',check_closeness)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(merged_data[\"Company\"], merged_data[\"GPT2 None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(merged_data[\"Company\"], merged_data[\"GPT4 None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross4_match = pd.crosstab(merged_data[\"Company\"], merged_data[\"SS=GPT4\"])\n",
    "cross4_match['Percentage'] = round((cross4_match[True] / (cross4_match[False] + cross4_match[True])) * 100,2)\n",
    "cross4_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross4_match = pd.crosstab(merged_data[\"Company\"], merged_data[\"SS#GPT4\"])\n",
    "cross4_match['Percentage'] = round((cross4_match[True] / (cross4_match[False] + cross4_match[True])) * 100,2)\n",
    "cross4_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross4_match = pd.crosstab(merged_data[\"Company\"], merged_data[\"SS>GPT4\"])\n",
    "cross4_match['Percentage'] = round((cross4_match[True] / (cross4_match[False] + cross4_match[True])) * 100,2)\n",
    "cross4_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(merged_data[\"Company\"], merged_data[\"SS=GPT2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross2_match = pd.crosstab(merged_data[\"Company\"], merged_data[\"SS=GPT2\"])\n",
    "cross2_match['Percentage'] = round((cross2_match[True] / (cross2_match[False] + cross2_match[True])) * 100,2)\n",
    "cross2_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross2_match = pd.crosstab(merged_data[\"Company\"], merged_data[\"SS>GPT2\"])\n",
    "cross2_match['Percentage'] = round((cross2_match[True] / (cross2_match[False] + cross2_match[True])) * 100,2)\n",
    "cross2_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rows = merged_data.loc[merged_data['SS Answer'] == merged_data['GPT Answer 4'], :]\n",
    "matching_dataframe = pd.DataFrame(matching_rows)\n",
    "matching_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_rows = merged_data.loc[merged_data['SS Answer'] != merged_data['GPT Answer 2'], :]\n",
    "non_matching_dataframe = pd.DataFrame(non_matching_rows)\n",
    "non_matching_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_greater_rows = merged_data.loc[merged_data['GPT Answer 2'] > merged_data['SS Answer']]\n",
    "gpt_greater_dataframe = pd.DataFrame(gpt_greater_rows)\n",
    "gpt_greater_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_greater_rows = merged_data.loc[merged_data['SS Answer'] > merged_data['GPT Answer 2']]\n",
    "ss_greater_dataframe = pd.DataFrame(ss_greater_rows)\n",
    "ss_greater_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_rows = merged_data.loc[merged_data['GPT Answer 2'].isnull()]\n",
    "none_dataframe = pd.DataFrame(none_rows)\n",
    "none_dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estnltk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
